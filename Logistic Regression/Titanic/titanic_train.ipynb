{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "a20a28e580a0151ea3c2581529936b24eb0b60772cb225affd8463079d4ed54d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS MODEL USES CS109's titanic dataset to learn and then predict whether you'd survive if you were on board the titanic in 1912."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#importing classes\n",
    "import copy\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn.model_selection as skm \n",
    "import matplotlib.pyplot as plt \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(709, 6)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"DataSets/titanic.csv\")\n",
    "y=data.Survived\n",
    "x=data.drop(['Name','Survived'],axis=1)\n",
    "x.loc[x.Sex==\"male\",\"Sex\"]=1\n",
    "x.loc[x.Sex==\"female\",\"Sex\"]=0\n",
    "x_train,x_test,y_train,y_test=[x.to_numpy() for x in skm.train_test_split(x,y,test_size=0.2)]\n",
    "print(x_train.shape)\n",
    "#offloading data from memory\n",
    "del x\n",
    "del y\n",
    "del data\n",
    "\n",
    "#using sklearn.model_selection we split the databases in test and train in 20%-80% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data to fit the proper value definitions:\n",
    "x_train=x_train.reshape(x_train.shape[0],-1).T\n",
    "x_test=x_test.reshape(x_test.shape[0],-1).T\n",
    "y_train=y_train.reshape(y_train.shape[0],-1).T\n",
    "y_test=y_test.reshape(y_test.shape[0],-1).T\n",
    "\n",
    "x_train=x_train/x_train.std()\n",
    "x_test=x_test/x_test.std()\n",
    "y_test=y_test/y_test.std()\n",
    "y_train=y_train/y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual development starts here\n",
    "#normal base functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    z=z.astype(float)\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializer\n",
    "def init(dim):\n",
    "    w=np.zeros((dim,1))\n",
    "    b=0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#propagation function used for forward and backward propagation\n",
    "def propagate(w,b,X,Y):\n",
    "    #forward pass \n",
    "\n",
    "    #creation of activation function\n",
    "    m=X.shape[1]\n",
    "    A=sigmoid(np.dot(w.T,X)+b) \n",
    "\n",
    "    #calculation of cost function\n",
    "\n",
    "    cost=-(1/m)*np.sum(((Y*np.log(A))+((1-Y)*np.log(1-A))))\n",
    "\n",
    "\n",
    "    #Backward Propagation: to find grad\n",
    "    dw=(np.dot(X,(A-Y).T))/m\n",
    "    db=(np.sum(A-Y))/m\n",
    "\n",
    "    cost=np.squeeze(np.array(cost))\n",
    "\n",
    "    grads={\n",
    "        \"dw\":dw,\n",
    "        \"db\":db\n",
    "    }\n",
    "\n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization function to run epochs and use transfer learning to update variables\n",
    "\n",
    "def optimize(w,b,X,Y,epochs=1000,learning_rate=0.5,view_cost=True):\n",
    "    costs=np.asarray([])    #np array to save costs\n",
    "    \n",
    "    for x in range(epochs):\n",
    "        grads,cost=propagate(w,b,X,Y)  #calculates 1 epoch and copies grads and cost here\n",
    "\n",
    "        #updating variables\n",
    "        \n",
    "        dw=grads[\"dw\"]\n",
    "        db=grads[\"db\"]\n",
    "\n",
    "        w=w-(learning_rate * dw) #the gradient is now used to updated\n",
    "        b=b-(learning_rate * db)\n",
    "\n",
    "        #printing out the cost function if view_cost=True:\n",
    "        if view_cost:\n",
    "            if x%200==0:\n",
    "                print(\"The cost after \",x,\"iterations is : \",cost)\n",
    "        \n",
    "    #storing the values\n",
    "\n",
    "    params={\n",
    "        \"w\":w,\n",
    "        \"b\":b\n",
    "    }\n",
    "\n",
    "    grads={\n",
    "        \"dw\":dw,\n",
    "        \"db\":db\n",
    "    }\n",
    "#returning the values after n epochs\n",
    "    return params,grads,costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the part where we try to predict the value\n",
    "\n",
    "def predict(w,b,X):\n",
    "    print(X.shape)\n",
    "    m=X.shape[1]\n",
    "    Y_predict=np.zeros((1,m))\n",
    "    #w=w.reshape(1,X.shape[1])\n",
    "\n",
    "    #reshaping and initializing the values\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "\n",
    "    #vectorized implementation : \n",
    "\n",
    "    Y_predict=(A>=0.5)*1.0\n",
    "\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final Function used to call all the values\n",
    "\n",
    "#The training is by default true , if its not training then set trainning = false during function call\n",
    "\n",
    "def model(X_train,Y_train,X_test,Y_test,epochs=1000,learning_rate=0.5,print_cost=True):\n",
    "    w,b=init(X_train.shape[0])\n",
    "\n",
    "    params,grads,costs=optimize(w,b,X_train,Y_train,epochs=epochs,learning_rate=learning_rate,view_cost=print_cost)\n",
    "\n",
    "    w=params[\"w\"]\n",
    "    b=params[\"b\"]\n",
    "\n",
    "    Y_predict_test=predict(w,b,Y_test)\n",
    "    Y_predict_train=predict(w,b,Y_train)\n",
    "\n",
    "    if(print_cost):\n",
    "        print(\"The Accuracy on test set: \",(100-np.mean(np.abs(Y_predict_test-Y_test))*100),\"%\")\n",
    "        print(\"The Accuracy on train set: \",(100-np.mean(np.abs(Y_predict_train-Y_train))*100),\"%\")\n",
    "\n",
    "    d={\n",
    "        \"w\":w,\n",
    "        \"b\":b\n",
    "        }\n",
    "    with open(\"learning_parameter.json\",\"w\") as write:\n",
    "        json.dumps(d,write)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The cost after  0 iterations is :  0.6931471805599453\n",
      "The cost after  200 iterations is :  0.341256317384601\n",
      "The cost after  400 iterations is :  0.1973188565550425\n",
      "The cost after  600 iterations is :  0.0834152396287583\n",
      "The cost after  800 iterations is :  -0.01988867166529872\n",
      "The cost after  1000 iterations is :  -inf\n",
      "<ipython-input-37-8f386d4603cd>:11: RuntimeWarning: divide by zero encountered in log\n",
      "  cost=-(1/m)*np.sum(((Y*np.log(A))+((1-Y)*np.log(1-A))))\n",
      "The cost after  1200 iterations is :  -inf\n",
      "The cost after  1400 iterations is :  -inf\n",
      "The cost after  1600 iterations is :  -inf\n",
      "The cost after  1800 iterations is :  -inf\n",
      "(1, 178)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (1,6) and (1,178) not aligned: 6 (dim 1) != 1 (dim 0)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-f1f5be094c76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-23ce49c561ce>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_cost)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mY_predict_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mY_predict_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-cf4340b50942>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(w, b, X)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#reshaping and initializing the values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#vectorized implementation :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,6) and (1,178) not aligned: 6 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "regression=model(x_train,y_train,x_test,y_test,epochs=2000,learning_rate=0.005,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}