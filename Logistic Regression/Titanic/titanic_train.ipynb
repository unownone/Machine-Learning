{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "a20a28e580a0151ea3c2581529936b24eb0b60772cb225affd8463079d4ed54d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS MODEL USES CS109's titanic dataset to learn and then predict whether you'd survive if you were on board the titanic in 1912."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#importing classes\n",
    "import copy\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn.model_selection as skm \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(709, 6)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"DataSets/titanic.csv\")\n",
    "y=data.Survived\n",
    "x=data.drop(['Name','Survived'],axis=1)\n",
    "x.loc[x.Sex==\"male\",\"Sex\"]=1\n",
    "x.loc[x.Sex==\"female\",\"Sex\"]=0\n",
    "x_train,x_test,y_train,y_test=[x.to_numpy() for x in skm.train_test_split(x,y,test_size=0.2)]\n",
    "print(x_train.shape)\n",
    "#offloading data from memory\n",
    "del x\n",
    "del y\n",
    "del data\n",
    "\n",
    "#using sklearn.model_selection we split the databases in test and train in 20%-80% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data to fit the proper value definitions:\n",
    "x_train=x_train.reshape(x_train.shape[0],-1).T\n",
    "x_test=x_test.reshape(x_test.shape[0],-1).T\n",
    "y_train=y_train.reshape(y_train.shape[0],-1).T\n",
    "y_test=y_test.reshape(y_test.shape[0],-1).T\n",
    "\n",
    "x_train=x_train/x_train.std()\n",
    "x_test=x_test/x_test.std()\n",
    "y_test=y_test/y_test.std()\n",
    "y_train=y_train/y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual development starts here\n",
    "#normal base functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializer\n",
    "def init(dim):\n",
    "    w=np.zeros((dim,1))\n",
    "    b=0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#propagation function used for forward and backward propagation\n",
    "def propagate(w,b,X,Y):\n",
    "    #forward pass \n",
    "\n",
    "    #creation of activation function\n",
    "    m=X.shape[1]\n",
    "    A=sigmoid(np.dot(w.T,X)+b) \n",
    "\n",
    "    #calculation of cost function\n",
    "\n",
    "    cost=-(1/m)*np.sum(((Y*np.log(A))+((1-Y)*log(1-A))))\n",
    "\n",
    "\n",
    "    #Backward Propagation: to find grad\n",
    "    dw=(np.dot(X,(A-Y).T))/m\n",
    "    db=(np.sum(A-Y))/m\n",
    "\n",
    "    cost=np.squeeze(np.array(cost))\n",
    "\n",
    "    grads={\n",
    "        \"dw\":dw,\n",
    "        \"db\":db\n",
    "    }\n",
    "\n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization function to run epochs and use transfer learning to update variables\n",
    "\n",
    "def optimize(w,b,X,Y,epochs=1000,learning_rate=0.5,view_cost=True):\n",
    "    costs=np.asarray([])    #np array to save costs\n",
    "    \n",
    "    for x in range(epochs):\n",
    "        grads,cost=propagate(w,b,X,Y)  #calculates 1 epoch and copies grads and cost here\n",
    "\n",
    "        #updating variables\n",
    "        \n",
    "        dw=grads[\"dw\"]\n",
    "        db=grads[\"db\"]\n",
    "\n",
    "        w=w-(learning_rate * dw) #the gradient is now used to updated\n",
    "        b=b-(learning_rate * db)\n",
    "\n",
    "        #printing out the cost function if view_cost=True:\n",
    "        if view_cost:\n",
    "            print(\"The cost after \",i,\"iterations is : \",cost)\n",
    "        \n",
    "    #storing the values\n",
    "\n",
    "    params={\n",
    "        \"w\":w,\n",
    "        \"b\":b\n",
    "    }\n",
    "\n",
    "    grads={\n",
    "        \"dw\":dw,\n",
    "        \"db\":db\n",
    "    }\n",
    "#returning the values after n epochs\n",
    "    return params,grads,costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the part where we try to predict the value\n",
    "\n",
    "def predict(w,b,X):\n",
    "\n",
    "    m=X.shape[1]\n",
    "    Y_predict=np.zeros((1,m))\n",
    "    w=w.reshape(X.shape[0],1)\n",
    "\n",
    "    #reshaping and initializing the values\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "\n",
    "    #vectorized implementation : \n",
    "\n",
    "    Y_predict=(A>=0.5)*1.0\n",
    "\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final Function used to call all the values\n",
    "\n",
    "#The training is by default true , if its not training then set trainning = false during function call\n",
    "\n",
    "def model(X_train=[],Y_train=[],X_test,Y_test,epochs=1000,learning_rate=0.5,learning=True,print_cost=True):\n",
    "    if learning:\n",
    "        \n",
    "        w,b=\n"
   ]
  }
 ]
}